# Task Management Planning API

This repository contains the initial scaffolding for an AI-assisted planning and PMO agent. It exposes a FastAPI application alongside helper modules for ingesting project data, validating rules, and exporting information.

## Project structure

```
task-management/
├─ src/
│  ├─ api/
│  │  └─ main.py           # FastAPI entrypoint
│  └─ pmo_agent/
│     ├─ __init__.py
│     ├─ schemas.py        # Pydantic models for project data
│     ├─ ingest.py         # CSV ingestion helpers
│     ├─ rules.py          # Validation and business rules
│     ├─ export.py         # Data export utilities
│     ├─ optimize.py       # Optimization logic (stub)
│     └─ explain.py        # Explainability logic (stub)
├─ data/                   # Place project CSVs here
├─ tests/                  # Test suite placeholder
├─ pyproject.toml          # Python project metadata and dependencies
├─ Dockerfile              # Container build instructions
└─ README.md
```

## Prerequisites

- Python 3.12+
- (Optional) Docker if you plan to run the application in a container.

## Local setup

1. **Create and activate a virtual environment** (recommended):

   ```bash
   python3.12 -m venv .venv
   source .venv/bin/activate
   ```

2. **Install dependencies** using the included `pyproject.toml`:

   ```bash
   pip install -U pip
   pip install .
   ```

3. **Run the FastAPI application** with Uvicorn:

   ```bash
   uvicorn src.api.main:app --host 0.0.0.0 --port 8000
   ```

4. Open <http://127.0.0.1:8000> in a browser or use `curl`/`httpie` to verify the health endpoint:

   ```bash
   curl http://127.0.0.1:8000/
   # {"status": "ok"}
   ```

5. (Optional) Launch the interactive API docs generated by FastAPI at <http://127.0.0.1:8000/docs>.

## Gemini LLM configuration

Set the `GEMINI_API_KEY` environment variable before running the application so the backend can authenticate with Google Gemini.
The service is configured to call the `models/gemini-2.0-flash-lite` model by default.

- **Locally (macOS/Linux):**

  ```bash
  export GEMINI_API_KEY="your-api-key"
  uvicorn src.api.main:app --host 0.0.0.0 --port 8000
  ```

- **Locally (Windows PowerShell):**

  ```powershell
  setx GEMINI_API_KEY "your-api-key"
  uvicorn src.api.main:app --host 0.0.0.0 --port 8000
  ```

- **Managed hosting (e.g., Render, HuggingFace Spaces):** Add `GEMINI_API_KEY` as a protected environment variable in your service settings.

### Example request

After starting the server, call the `/llm` endpoint with a JSON body containing your prompt:

```bash
curl -X POST http://127.0.0.1:8000/llm \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Summarize the benefits of using Gemini."}'
# {"response": "...model output..."}
```

## Using Docker

1. **Build the image**:

   ```bash
   docker build -t task-management .
   ```

2. **Run the container** (Render and HuggingFace Spaces compatible):

   ```bash
   docker run -p 7860:7860 -e PORT=7860 task-management
   ```

   The server listens on the `PORT` environment variable if supplied; otherwise it defaults to `7860`.

## Render deployment notes

- Render automatically injects a `PORT` environment variable into Docker deployments. The startup command in the Dockerfile uses shell expansion (`${PORT:-7860}`) so the service binds to that value at runtime.
- The `EXPOSE 7860` line in the Dockerfile is only metadata for local tooling (e.g., Docker Desktop, HuggingFace Spaces). Render ignores it and maps the internal container port specified by `PORT` to the public endpoint automatically, so no additional configuration is required.
- If you override the start command in Render, keep the `--port $PORT` flag so Uvicorn continues to respect the injected value.

## Next steps

- Populate the `data/` directory with project CSV files for ingestion.
- Add unit tests under `tests/` as features are implemented.
- Implement the optimization and explainability modules to flesh out the PMO agent capabilities.

## License

This project is provided as-is; update this section with licensing details if required for your deployment.
